<?xml version="1.0" encoding="utf-8" ?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>fork()</title>
    <atom:link href="http://ripsawridge.github.io//feed.xml" rel="self" type="application/rss+xml"></atom:link>
    <link>http://ripsawridge.github.io/</link>
    <description>Notes from my endeavors</description>
    <pubDate>Tue, 25 Apr 2017 02:00:00 +0200</pubDate>
    <generator>Wintersmith - https://github.com/jnordberg/wintersmith</generator>
    <language>en</language>
    <item>
      <title>Poking around in Speedometer</title>
      <link>http://ripsawridge.github.io//articles/blink-mysterium/</link>
      <pubDate>Tue, 25 Apr 2017 02:00:00 +0200</pubDate>
      <guid isPermaLink="true">http://ripsawridge.github.io//articles/blink-mysterium/</guid>
      <author>Michael Stanton</author>
      <description>&lt;p&gt;My marching orders today were:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;OPPORTUNITIES FOR SMALL IMPROVEMENT IN JQUERY SPEEDOMETER BNCHMRK. STOP.
WHY CALLS TO RUNTIME KEYED LOAD? WHY CALLS TO HANDLEAPICALL? WHY? STOP.&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;These days I’m a manager type, more comfortable droning on in sonorous tones
with made up words like “leveraging” and “embiggen,” but p’raps I can saddle
up and look at the code again.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;First! To use Jaro’s new ProfView tool! (&lt;code&gt;v8/tools/profview/index.html&lt;/code&gt;).
Hilariously, I was poking around in /tools and found &lt;code&gt;profviz&lt;/code&gt;, an apparently
abandoned tool that Jaro didn’t even know about. “As a manager, I encourage you
to leverage existing resources for maximum effect.”&lt;/p&gt;
&lt;p&gt;He just stared at me.&lt;/p&gt;
&lt;p&gt;Okay! Back to work. First I run the Speedometer jQuery benchmark in chrome like
this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;out/Release/chrome --no-sandbox --js-flags=&amp;quot;--prof&amp;quot;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;I have to use &lt;code&gt;--no-sandbox&lt;/code&gt; so that an &lt;code&gt;isolate-v8.log&lt;/code&gt; file can be written to disk.
And &lt;code&gt;--js-flags&lt;/code&gt; is the place to pass V8 options.&lt;/p&gt;
&lt;center&gt;&lt;a href=&quot;images/interactiverunnger.jpg&quot;&gt;&lt;img src=&quot;images/interactiverunner.jpg&quot; width=600&gt;&lt;/a&gt;&lt;/center&gt;

&lt;p&gt;I ran the test three times, shut Chrome down, then process the log file.
Oh jeez, there are four of them. Sigh. I’ll try the first one. Running in the
chrome directory:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;v8/tools/linux-tick-processor --preprocess isolate-0x20c3d10ee000-v8.log &amp;gt;
    mystery.json&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;I load that file up in ProfView, and see the following:&lt;/p&gt;
&lt;center&gt;&lt;a href=&quot;images/profview1.jpg&quot;&gt;&lt;img src=&quot;images/profview1.jpg&quot; width=600&gt;&lt;/a&gt;&lt;/center&gt;

&lt;p&gt;The first issue is from the function &lt;code&gt;cleanData&lt;/code&gt;, where a keyed load ends up in
the runtime both from an unoptimized version of &lt;code&gt;cleanData&lt;/code&gt; and an optimized
version. Wha? Can’t we do better? Adding insult to injury,
&lt;code&gt;Runtime_KeyedGetProperty&lt;/code&gt; calls Blink’s &lt;code&gt;indexedPropertyGetterCallback&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;HEADQRTRS THOUGHT WE HAD A FAST PATH FOR THAT.
STOP.&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Here is the code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;cleanData: &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;function&lt;/span&gt;(&lt;span class=&quot;params&quot;&gt; elems, &lt;span class=&quot;regexp&quot;&gt;/* internal */&lt;/span&gt; acceptData &lt;/span&gt;) &lt;/span&gt;{
  &lt;span class=&quot;keyword&quot;&gt;var&lt;/span&gt; elem, type, id, data, i = &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;,
    internalKey = jQuery.expando,
    cache = jQuery.cache,
    deleteExpando = jQuery.support.deleteExpando,
    special = jQuery.event.special;

  &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; ( ; (elem = elems[i]) != &lt;span class=&quot;literal&quot;&gt;null&lt;/span&gt;; i++ ) {

    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; ( acceptData || jQuery.acceptData( elem ) ) {
      id = elem[ internalKey ];
      data = id &amp;amp;&amp;amp; cache[ id ];
      &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; ( data ) {
        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; ( data.events ) {
           ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well, I see at least 3 keyed loads in the code above, and there are even more below.
Hmm, ProfView can’t tell me &lt;em&gt;which&lt;/em&gt; keyed load is the expensive one. I’ll use Chrome
Developer Tools to take a snapshot of a run to try and get more hints…&lt;/p&gt;
&lt;center&gt;&lt;a href=&quot;images/devtools1.jpg&quot;&gt;&lt;img src=&quot;images/devtools1.jpg&quot; width=600&gt;&lt;/a&gt;&lt;/center&gt;

&lt;p&gt;The line &lt;code&gt;data = id &amp;amp;&amp;amp; cache[ id ]&lt;/code&gt; gets most of the ticks. It’s reasonable to conclude
this is the heaviest load. In fact though, it’s the lightest.&lt;/p&gt;
&lt;p&gt;Time to visit my old friend, &lt;code&gt;--trace-ic&lt;/code&gt; and watch what happens to this keyed load ic.
Things have changed in the old neighborhood…the output no longer goes to the console, but
the log file. M’okay.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;v8/tools/ic-processor isolate-0x5acebee7000-v8.log &amp;gt; mystery-ic.txt

$ cat mystery-ic.txt | grep KeyedLoadIC | grep cleanData
KeyedLoadIC (0-&amp;gt;1) at cleanData ../jquery.js:6572:27 0 (map 0x20d03abb5aa9)
KeyedLoadIC (0-&amp;gt;.) at cleanData ../jquery.js:6576:16 jQuery19101482418194915336 (map 0x20d03abcffc1)
KeyedLoadIC (.-&amp;gt;1) at cleanData ../jquery.js:6576:16 jQuery19101482418194915336 (map 0x20d03abb5529)
KeyedLoadIC (1-&amp;gt;N) at cleanData ../jquery.js:6576:16 jQuery19101482418194915336 (map 0x20d03abd0281)
KeyedLoadIC (1-&amp;gt;1) at cleanData ../jquery.js:6572:27 0 (map 0x20d03aba4121)
KeyedLoadIC (1-&amp;gt;N) at cleanData ../jquery.js:6572:27 1 (map 0x20d03aba4121)
KeyedLoadIC (N-&amp;gt;N) at cleanData ../jquery.js:6576:16 jQuery19101482418194915336 (map 0x20d03abcffc1)
KeyedLoadIC (N-&amp;gt;N) at cleanData ../jquery.js:6576:16 jQuery19101482418194915336 (map 0x20d03abd5191)
KeyedLoadIC (N-&amp;gt;N) at cleanData ../jquery.js:6576:16 jQuery19101482418194915336 (map 0x20d03abb55d9)
KeyedLoadIC (N-&amp;gt;N) at cleanData ../jquery.js:6576:16 jQuery19101482418194915336 (map 0x20d03abb62e9)
KeyedLoadIC (N-&amp;gt;N) at cleanData ../jquery.js:6576:16 jQuery19101482418194915336 (map 0x20d03abd53f9)
KeyedLoadIC (0-&amp;gt;1) at cleanData ../jquery.js:6577:25 14 (map 0x20d03aba2ee9)
KeyedLoadIC (0-&amp;gt;1) at cleanData ../jquery.js:6593:17 14 (map 0x20d03aba2ee9)
KeyedLoadIC (N-&amp;gt;N) at cleanData ../jquery.js:6576:16 jQuery19101482418194915336 (map 0x20d03abb55d9)
KeyedLoadIC (N-&amp;gt;N) at cleanData ../jquery.js:6576:16 jQuery19101482418194915336 (map 0x20d03abb62e9)
KeyedLoadIC (N-&amp;gt;N) at cleanData ../jquery.js:6576:16 jQuery19101482418194915336 (map 0x20d03abd53f9)
KeyedLoadIC (N-&amp;gt;N) at cleanData ../jquery.js:6576:16 jQuery19101482418194915336 (map 0x20d03abd0281)
KeyedLoadIC (N-&amp;gt;N) at cleanData ../jquery.js:6576:16 jQuery19101482418194915336 (map 0x20d03abcffc1)
KeyedLoadIC (N-&amp;gt;N) at cleanData ../jquery.js:6576:16 jQuery19101482418194915336 (map 0x20d03abb5529)
...&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Hmm. It’s sad to see that for loop with &lt;code&gt;elem = elems[i]&lt;/code&gt; go generic. That’s weird. I also notice that
I can’t tell from this log output if the IC is in the optimized version of cleanData or the unoptimized
version. The old trace-ic logs used to display this.
&lt;a href=&quot;https://codereview.chromium.org/2835923004/&quot;&gt;I’ll just take care of that now, yallz&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Turning away from IC tracing for a moment, let’s look at the optimized code for &lt;code&gt;cleanData&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;out/Release/chrome --no-sandbox --js-flags=&amp;quot;--trace-opt-code --trace-opt&amp;quot; &amp;gt; code.txt&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now I can look at the optimized code for &lt;code&gt;cleanData&lt;/code&gt; and see when keyed load ICs are used.
At the risk (certainty!) of overbroad generalization, if we use an IC in optimized code it
&lt;em&gt;means we’ve given up trying to optimize it&lt;/em&gt;. The code is
“generic” from our point of view. Hopefully we can do better from this point.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    -- B4 start (loop up to 160) --
0x2e72378a7380   120  488b4dd8       REX.W movq rcx,[rbp-0x28]
0x2e72378a7384   124  48c1e120       REX.W shlq rcx, 32
    -- ../jquery.js:6572:27 --
0x2e72378a7388   128  488b75f8       REX.W movq rsi,[rbp-0x8]
0x2e72378a738c   12c  488b5518       REX.W movq rdx,[rbp+0x18]
0x2e72378a7390   130  48b8000000000e000000 REX.W movq rax,0xe00000000
0x2e72378a739a   13a  4c8bc0         REX.W movq r8,rax
0x2e72378a739d   13d  e89e8ef2ff     call 0x2e72377d0240  (KeyedLoadICTrampoline)
    ;; code: KEYED_LOAD_IC&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Yuck, this is the for loop with &lt;code&gt;elem = elems[i]&lt;/code&gt;. So sad that we are generic! I really want to know why.
I ran chrome with the debugger, first adjusting &lt;code&gt;gn args out/Debug&lt;/code&gt; to include the key
&lt;code&gt;v8_optimized_debug = false&lt;/code&gt; so I could inspect variables. I found the place in the IC system where we
update the IC state, using printf-style debugging to stop at the right place:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;
&lt;span class=&quot;comment&quot;&gt;// in src/ic/ic.cc:&lt;/span&gt;

&lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; KeyedLoadIC::UpdateLoadElement(Handle&amp;lt;HeapObject&amp;gt; receiver) {
  ...

  &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;built_in&quot;&gt;strcmp&lt;/span&gt;(&lt;span class=&quot;string&quot;&gt;&quot;cleanData&quot;&lt;/span&gt;,
             GetHostFunction()-&amp;gt;shared()-&amp;gt;DebugName()-&amp;gt;ToCString().get()) == &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;) {
    PrintF(&lt;span class=&quot;string&quot;&gt;&quot;This is your stop.\n&quot;&lt;/span&gt;);
  }
  ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&quot;https://chromium.googlesource.com/chromium/src/+/lkgr/docs/linux_debugging.md&quot;&gt;Debugging Chrome&lt;/a&gt;
can be tricky, here is the command line I used:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gdb --args out/Debug/chrome --disable-gpu --single-process --no-sandbox
    http://browserbench.org/Speedometer/InteractiveRunner.html&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Setting a breakpoint on the &lt;code&gt;PrintF&lt;/code&gt; statement, I first hit it when we went monomorphic.
Then we come in with a new map and stay monomorphic. On the third hit we go generic with
the message &lt;code&gt;same map added twice.&lt;/code&gt; I could inspect the receiver and the feedback vector
with our friendly debugger helpers (available in &lt;code&gt;v8/tools/gdbinit&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(gdb) frame 2
#2  0x00007f2a77e8a2d1 in v8::internal::__RT_impl_Runtime_KeyedLoadIC_Miss (args=...,
    isolate=0xe93d3eb9020) at ../../v8/src/ic/ic.cc:2439
2439      RETURN_RESULT_OR_FAILURE(isolate, ic.Load(receiver, key));
(gdb)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In this frame I can look at the input variables. The receiver:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(gdb) job *receiver
0x9cf0a77a209: [JSArray]
 - map = 0x3114690a4121 [FastProperties]
 - prototype = 0x3f6568a88141
 - elements = 0x9cf0a77a239 &amp;lt;FixedArray[1]&amp;gt; [FAST_ELEMENTS]
 - length = 1
 - properties = 0x61c9ec02241 &amp;lt;FixedArray[0]&amp;gt; {
    #length: 0x38b9c858c59 &amp;lt;AccessorInfo&amp;gt; (const accessor descriptor)
 }&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Hmm, we have an elements store of length 1. What about our key?&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(gdb) p key
$8 = {&amp;lt;v8::internal::HandleBase&amp;gt; = {location_ = 0x7f2a495e8920}, &amp;lt;No data fields&amp;gt;}
(gdb) job *key
 Smi: 0x1 (1)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Oh. The key is a Smi with the value 1. Sigh. We are looking at the array beyond the end. And a quick
glance at the KeyedLoadIC dispatcher shows that we MISS to the runtime in this case (this code is in
&lt;code&gt;AccessorAssembler::HandleLoadICSmiHandlerCase()&lt;/code&gt;, I won’t go into it here but it has a telling
line of code…&lt;code&gt;Label* out_of_bounds = miss;&lt;/code&gt; LOL. This means out of bounds access results in a trip
to the runtime. So you see, v8 is not optimized to clever code like
&lt;code&gt;for ( ; (elem = elems[i]) != null; i++ ) {&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;I locally changed the code to look more old-fashioned because I wanted to see the KeyedLoadIC go away.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;cleanData: &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;function&lt;/span&gt;(&lt;span class=&quot;params&quot;&gt; elems, &lt;span class=&quot;regexp&quot;&gt;/* internal */&lt;/span&gt; acceptData &lt;/span&gt;) &lt;/span&gt;{
  ...
  var len = elems.length;
  &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; ( ; i &amp;lt; len; i++ ) {
    elem = elems[i];
    ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;My change made the keyed load
run in polymorphic 2-degree state. Why wasn’t this enough to emit optimized loads? Time to debug TurboFan. We have code
in &lt;code&gt;JSNativeContextSpecialization::ReduceElementAccess()&lt;/code&gt; which builds the polymorphic loads
keying on the receiver map. I stepped into this…&lt;/p&gt;
&lt;p&gt;A look at the feedback vector made me realize it wouldn’t be so simple. One of the maps we had to
handle is an indexed interceptor.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(gdb) jfv *nexus.vector_handle_
0x5932c9d0ff1: [FeedbackVector]
 - length: 66
 Slot #0 LoadProperty MONOMORPHIC
 ...
 Slot #16 LoadKeyed POLYMORPHIC
  [18]: 0xfda86d07429 &amp;lt;FixedArray[4]&amp;gt;
  [19]: 0x1428fdb04e21 &amp;lt;Symbol: uninitialized_symbol&amp;gt;
 Slot #18 Call MONOMORPHIC
 ...
(gdb) job 0xfda86d07429
0xfda86d07429: [FixedArray]
 - map = 0xfca2f802309 &amp;lt;Map(FAST_HOLEY_ELEMENTS)&amp;gt;
 - length: 4
           0: 0x1ec5780083f9 &amp;lt;WeakCell value= 0x3ba1419a4121 &amp;lt;Map(FAST_ELEMENTS)&amp;gt;&amp;gt;
           1: 72
           2: 0x1ae5bf10de49 &amp;lt;WeakCell value= 0x3ba1419b5b01 &amp;lt;Map(FAST_HOLEY_SMI_ELEMENTS)&amp;gt;&amp;gt;
           3: 0x29f74c85261 &amp;lt;Code HANDLER&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;What this means is that we have a polymorphic keyed load with smi-like keys (the uninitialized
symbol in &lt;code&gt;[19]&lt;/code&gt; tells us this). The fixed array is a mapping of the receiver map to a handler.
Sometimes the handler is code, sometimes it’s a smi telling us what to do. A look at the code
shows that it’s the indexed interceptor handler:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(gdb) jco 0x29f74c85261
0x29f74c85261: [Code]
kind = HANDLER
major_key = LoadIndexedInterceptorStub
compiler = turbofan
Instructions (size = 92)
... (I removed some stuff) ...
0x29f74c85318    38  48bb609731d0d17f0000 REX.W movq rbx,0x7fd1d0319760
    ;; external reference (Runtime::LoadElementWithInterceptor)
0x29f74c85322    42  b802000000     movl rax,0x2
0x29f74c85327    47  4889542410     REX.W movq [rsp+0x10],rdx
0x29f74c8532c    4c  e9cff1dfff     jmp 0x29f74a84500 ;; code: STUB, CEntryStub, minor: 8
// Call LoadElementWithInterceptor through the CEntryStub.&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Back in &lt;code&gt;JSNativeContextSpecialization::ReduceElementAccess()&lt;/code&gt;, I see the writing on the wall.
We only do fast handling of polymorphic keyed element loads if all the Maps are simple fast
element arrays. We call &lt;code&gt;AccessInfoFactory::ComputeElementAccessInfos()&lt;/code&gt; which runs a test
on each Map that succinctly expresses what we consider reasonable to inline:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;CanInlineElementAccess&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(Handle&amp;lt;Map&amp;gt; &lt;span class=&quot;built_in&quot;&gt;map&lt;/span&gt;)&lt;/span&gt; &lt;/span&gt;{
  &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; (!&lt;span class=&quot;built_in&quot;&gt;map&lt;/span&gt;-&amp;gt;IsJSObjectMap()) &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;false&lt;/span&gt;;
  &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;built_in&quot;&gt;map&lt;/span&gt;-&amp;gt;is_access_check_needed()) &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;false&lt;/span&gt;;
  &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;built_in&quot;&gt;map&lt;/span&gt;-&amp;gt;has_indexed_interceptor()) &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;false&lt;/span&gt;;  &lt;span class=&quot;comment&quot;&gt;// Tears of a clown...&lt;/span&gt;
  ElementsKind &lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; elements_kind = &lt;span class=&quot;built_in&quot;&gt;map&lt;/span&gt;-&amp;gt;elements_kind();
  &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; (IsFastElementsKind(elements_kind)) &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;true&lt;/span&gt;;
  &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; (IsFixedTypedArrayElementsKind(elements_kind)) &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;true&lt;/span&gt;;
  &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;false&lt;/span&gt;;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So I know that the &lt;code&gt;elem = elems[i]&lt;/code&gt; code is generic for two reasons: out of bounds access and the use of
an indexed interceptor on one of the maps. I waddled over to &lt;a href=&quot;https://twitter.com/tverwaes&quot;&gt;Toon’s&lt;/a&gt; desk (they feed us a lot here,
jus’ sayin), and asked him about the possibility of optimizing indexed interceptor access. He pointed out that
even if we were able to avoid going through the IC handler that makes the runtime call, we are still
on our way into C++ on the Blink side. For now then, I’ll quit trying to optimize this place in the code.&lt;/p&gt;
&lt;h2 id=&quot;moving-on-to-id-elem-internalkey-&quot;&gt;Moving on to &lt;code&gt;id = elem[ internalKey ]&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;internalKey&lt;/code&gt; is a string like “jQuery1910…”. The receiver is a &lt;code&gt;JS_API_OBJECT_TYPE&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(gdb) job *object
0x3c19691f4d91: [JS_API_OBJECT_TYPE]
 - map = 0xf0136cb4f51 [FastProperties]
 - prototype = 0x230e375a0009
 - elements = 0x1a845fd82241 &amp;lt;FixedArray[0]&amp;gt; [FAST_HOLEY_SMI_ELEMENTS]
 - embedder fields: 2
 - properties = 0x1a845fd82241 &amp;lt;FixedArray[0]&amp;gt; {}
 - embedder fields = {
    0x7fe301dfb650
    0x1046fdd4ef80
 }&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;A comment in &lt;code&gt;objects.h&lt;/code&gt; sez “Like JS_OBJECT_TYPE but created from Api functions.” Cool.&lt;/p&gt;
&lt;p&gt;Anyway, in the IC system we don’t find the property on the object, so we prepare for a non-existent
load. We create a &lt;code&gt;Tuple3&lt;/code&gt; which is a struct with three values: a holder cell, a smi-handler and a validity cell.
This information makes it possible to handle the load as efficiently as possible while safeguarding against
structural changes that invalidate the approach.&lt;/p&gt;
&lt;p&gt;The smi-handler value is 0x7, which means it’s a handler meant to “load” non-existent properties. Generally,
this means returning undefined to the caller. The validity cell protects against this ever changing.&lt;/p&gt;
&lt;p&gt;How did I find all this out? By poking around in the feedback vector. I did this poking around after the
first miss, so the IC is in monomorphic state at this point. It’s only later we’ll go megamorphic.
Here is the feedback vector slot for our keyed load:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(gdb) jfv nexus_-&amp;gt;vector()
0x671f59d0e21: [FeedbackVector]
 - length: 64
....
 Slot #20 LoadKeyed MONOMORPHIC
   [22]: 0x231997de8b99 &amp;lt;String[27]: jQuery191044337671379997645&amp;gt;
   [23]: 0x2a9781fd401 &amp;lt;FixedArray[2]&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and here is the data with the &lt;code&gt;Tuple3&lt;/code&gt; structure, as pulled out of the FixedArray in &lt;code&gt;[23]&lt;/code&gt; above:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(gdb) job 0x2a9781fd401
0x2a9781fd401: [FixedArray]
 - map = 0x1a6253c82309 &amp;lt;Map(FAST_HOLEY_ELEMENTS)&amp;gt;
 - length: 2
 0: 0x231997de25f9 &amp;lt;WeakCell value= 0xad329834ef9 &amp;lt;Map(FAST_HOLEY_SMI_ELEMENTS)&amp;gt;&amp;gt;
 1: 0x671f59d63e1 &amp;lt;Tuple3&amp;gt;
(gdb) job 0x671f59d63e1
0x671f59d63e1: [Tuple3]
 - value1: 0x3e7913e02201 &amp;lt;null&amp;gt;
 - value2: 7
 - value3: 0x231997de25d1 &amp;lt;Cell value= 0&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So this load is quite complicated already, and soon goes Megamorphic because we come in with different receiver maps.&lt;/p&gt;
&lt;h2 id=&quot;anything-else-to-do-&quot;&gt;Anything else to do?&lt;/h2&gt;
&lt;p&gt;The last keyed load I’m interested in is for the line &lt;code&gt;data = id &amp;amp;&amp;amp; cache[ id ]&lt;/code&gt;. &lt;code&gt;cache&lt;/code&gt; is the JQuery cache
object, just an ordinary object with some 150 holey array elements. This keyed load is monomorphic and uses a simple
element handler. Turbofan should definitely optimize this. In the vector we store the map of the JQuery
cache, and have a smi value to represent what we should do. It encodes whether or not the object is a JSArray
(it is not), and what the ElementsKind is (&lt;code&gt;FAST_HOLEY_ELEMENTS&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Happily, this load has been optimized in TurboFan.&lt;/p&gt;
&lt;p&gt;Unhappily for me, I don’t have an A-HA! discovery in this block to crow over. Performance investigations
sometimes are like that.&lt;/p&gt;
&lt;p&gt;As for the &lt;code&gt;Builtin_HandleApiCall&lt;/code&gt; issue in unoptimized JavaScript function &lt;code&gt;curCSS&lt;/code&gt; in the output from Jaro’s
tool, &lt;a href=&quot;https://twitter.com/bmeurer&quot;&gt;Benedikt&lt;/a&gt; did some follow-up there, recognizing cases where we could use a stub
to make the api call less expensive. A story for another day…&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Altering JavaScript frames</title>
      <link>http://ripsawridge.github.io//articles/stack-changes/</link>
      <pubDate>Sat, 21 Feb 2015 01:00:00 +0100</pubDate>
      <guid isPermaLink="true">http://ripsawridge.github.io//articles/stack-changes/</guid>
      <author>Michael Stanton</author>
      <description>&lt;p&gt;For a while I’ve been working on a project in &lt;a href=&quot;https://code.google.com/p/v8/&quot;&gt;V8&lt;/a&gt; to encode type feedback into
simple data structures rather than embedding it in compiled code.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The V8 inline
cache system typically compiles a “dispatcher” which checks an incoming object
map against a constant. If there is a match, control is dispatched to a handler,
which may be a stock stub or be specially compiled for this object. The inline
cache (IC) patches this dispatcher code into a compiled function. The dispatcher
improves performance, because many decisions have been reduced to a comparison
of a map against a constant (we call this a map check). We can also examine
the dispatcher later for it’s embedded maps to determine what it knows when
creating optimized code.&lt;/p&gt;
&lt;p&gt;After this thumbnail sketch of how ICs work (here is &lt;a href=&quot;http://mrale.ph/blog/2012/06/03/explaining-js-vms-in-js-inline-caches.html&quot;&gt;a much better one&lt;/a&gt;), you may think, why change it? Well,
it would be nice to avoid patching code for security reasons and the fact that
it causes a flush of the instruction cache which hampers performance on some
platforms. Storing our maps in arrays is natural and makes extending the
information we collect easier. For example, we might want to store polymorphic
call counts. When we use a data structure, we can just store a triple for each
map: the map, the “handler” that we jump to, and finally an integer
count. That could be used later to order polymorphic calls. You might even
coalesce this data by shunting rarely used maps to a generic handler and
therefore reduce the degree of polymorphism.&lt;/p&gt;
&lt;p&gt;So that’s why it would be nice to embed information in data structures rather
than code. But the V8 IC system is rich, complex and performance
sensitive. Becuase of that introducing data structures for feedback has been
slow. A year ago I began using the “type feedback vector” to record data for
calls from one JavaScript function to another. Now I’m working on making loads
(like &lt;code&gt;x = obj.foo&lt;/code&gt; and keyed loads like &lt;code&gt;x = obj[h]&lt;/code&gt;) use the type
feedback vector, and avoid patching code completely.&lt;/p&gt;
&lt;p&gt;It’s difficult because a data structure solution means more memory loads no
matter how you slice it. Here we come to another potential beneft of a type
vector: it could be used in optimized code for which we only have partial type
feedback. Normally, V8 will deoptimize an optimized function if it begins
running a section for which we never ran before in full code. This could happen
if the function is considered “hot” but there is a branch that was never yet
taken. With the type feedback vector, we could install vector-based ICs in those
information-poor locations, allow them to learn for a while, then reoptimize
after achieving a certain threshold of new information.&lt;/p&gt;
&lt;p&gt;Deoptimizing functions is expensive for V8, and I’ll go into that more later -
it’s just a tremendous amount of work and complexity. Type vectors offer the
possibility to smooth out and moderate the optimized/un-optimized transition
curve over the lifetime of an application.&lt;/p&gt;
&lt;p&gt;So that is my motivation. V8 is using the type vector for call ICs as mentioned,
but loads are the important case because there are so many of them. If that can
be achieved, then we have license to go the rest of the way and eliminate
patching entirely. It’s a tremendously fun project.&lt;/p&gt;
&lt;p&gt;I’ve been writing this document as I learned about the area, and was inspired by 
&lt;a href=&quot;http://mrale.ph/blog/2012/06/03/explaining-js-vms-in-js-inline-caches.html&quot;&gt;Vyacheslav Egorov’s article explaining inline
caches&lt;/a&gt;
in a readable and entertaining way. I loved the way his drawings looked, as it
reminded me of the only way I seem to be able to internalize most concepts: by
drawing them on paper. Vyacheslav built &lt;a href=&quot;https://moe-js.googlecode.com/git/talks/jsconfeu2012/tools/shaky/deploy/shaky.html&quot;&gt;a
tool&lt;/a&gt;
to create attractive “box and pointer” drawings from ASCII, and I
started using it to think about the process along the way. Creating these
pictures because a major part of the fun in the last few days :D.&lt;/p&gt;
&lt;h2 id=&quot;too-many-loads&quot;&gt;Too many loads&lt;/h2&gt;
&lt;p&gt;I’ve already spent time micro-optimizing my data-driven dispatcher, which
grovels about in the vector to complete it’s map-checks and dispatches. That is
the subject of another article, but suffice to say here that when I’m
contemplating doing 2 levels of speculative reads into data carefully
constructed to guarantee crash avoidance, just in order to save one additional
read…I’ve probably hit the end of the line for that activity.&lt;/p&gt;
&lt;p&gt;Now I turn to the number of reads required before the call to the
dispatcher. The type vector is an array attached to the SharedFunctionInfo for a
JavaScript function. It’s indexed by a “slot,” and these slots are handed out
at compilation time to compilation nodes that request them. The IC receives a pointer to the
vector and an integer index into the vector (the index is derived from the slot
but not the same thing).&lt;/p&gt;
&lt;p&gt;Fair enough, but how do we load the vector into a register for the call? I could
just embed it in the code, as it’s a constant, but experimentally, this changed
the code size so much even just when using it for calls, that it would bloat the
code unacceptably if I do this for all IC types. It threw off our profiler
calculations, highlighting a weakness there that the profiler is based on code
size in bytes rather than say, number of abstract syntax tree nodes (this should be tackled and
solved, of course!). What proved a better solution for production was a series
of loads. The &lt;code&gt;JSFunction&lt;/code&gt; associated with this function is available in the
stack frame. I load that, then walk through to the vector hanging off the
SharedFunctionInfo. It seems that these loads aren’t too expensive because the
data is in the cache.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;drawings/frame-jsfunction.png&quot; width=600&gt;&lt;/center&gt;

&lt;p&gt;But for wider deployment of the type vector concept, this many loads becomes
hard to support. Consider function &lt;strong&gt;foo&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;foo&lt;/span&gt;(&lt;span class=&quot;params&quot;&gt;obj, x&lt;/span&gt;) &lt;/span&gt;{
  &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;keyword&quot;&gt;var&lt;/span&gt; i = &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;; i &amp;lt; x.length; i++) {
    x[i] = x[i] * i + obj.foo;
    check(i);
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The expressions &lt;code&gt;x.length&lt;/code&gt;, the second &lt;code&gt;x[i]&lt;/code&gt;, &lt;code&gt;obj.foo&lt;/code&gt;, and
  &lt;code&gt;check(i)&lt;/code&gt; all need the type vector. Just considering that the vector needs 3
loads, that is &lt;code&gt;3 * 4 * x.length&lt;/code&gt; loads.&lt;/p&gt;
&lt;p&gt;Ideally, we would just have 3 loads, by hoisting the vector load out of the
loop. But that involves more architecture than we want to invest in full
code. Usage of the type vector in optimized code isn’t supposed to be very
heavy, but by introducing the vector as a node in those compilations we’ll get
that kind of hoisting there. But I can reduce the number of loads by storing the
feedback vector in the frame, meaning we’ll have &lt;code&gt;4 * x.length&lt;/code&gt; loads (or at
least until the profiler decides the function is hot enough and drops in an
optimized version in place via on-stack-replacement (OSR), which is a fantastic
thing). 
Whats more, these loads are all from a stack address in the frame and should remain in
the cache. &lt;/p&gt;
&lt;p&gt;This means I’ll have to alter the frame layout. Gulp.&lt;/p&gt;
&lt;h2 id=&quot;unoptimized-javascript-frames-get-a-vector&quot;&gt;Unoptimized JavaScript frames get a Vector&lt;/h2&gt;
&lt;p&gt;First off, why only add the vector to unoptimized JavaScript frames? Well, an
optimized JavaScript frame actually contains many vectors, one for each function
that it inlines. The vector for the ostensibly optimized function is only
partially useful, and couldn’t be referred to by any of the inlined
functions. Of course, there could be a load/restore step surrounding inlined
calls, but that seems like a lot of work in code that should be tight, and
ideally, shouldn’t use the type vector at all. Ideally we’ve learned from all
ICs seen thus far. Also, if we need to refer to a type feedback vector in
optimized code, we could let sophisticated technologies like GVN and the
register allocator decide where to put the constant vector address and when to
load it.&lt;/p&gt;
&lt;p&gt;Therefore, here is a V8 JavaScript Frame, with a type vector field added just
after the JSFunction. The stack is positioned just before making a call to
another function:&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;drawings/frame1.png&quot; width=600&gt;&lt;/center&gt;

&lt;p&gt;An optimized frame looks a bit different. There is no vector, but there is an
alignment word on 32 bit platforms that indicates whether the stack has been
aligned or not. Here is a case where no alignment occurred, and just before a
call to another function:&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;drawings/frame2.png&quot; width=600&gt;&lt;/center&gt;

&lt;p&gt;Alignment introduces some complication. When we are about to save the previous
&lt;code&gt;$ebp&lt;/code&gt; to the stack, we check to see if &lt;code&gt;$esp&lt;/code&gt; is aligned. If so, we proceed
normally, saving the value 0 in the alignment slot in the frame. Otherwise,
we’ll move the receiver, arguments and return address down one word on the
stack, putting a “zap value” (&lt;code&gt;0x12345678&lt;/code&gt;) where the receiver used to be. Then in the
alignment slot we’ll store the value 2 as a signal when it’s time to dismantle
the frame. When we encounter that value on return, we know we need to clear one
more word from the stack on return (the “zap value”). We have to read the
alignment slot before we dismantle the frame, then after taking the frame down
we have to take care of the receiver and arguments. The example below is a
function with one argument, and a receiver. The
optimized frame just has one real spill slot, the other is reserved for the
alignment word.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;drawings/frame-align.png&quot; width=600&gt;&lt;/center&gt;

&lt;p&gt;The need for alignment of an optimized frame is recognized on entry, before
setting up the frame. A “zap value” is inserted and the stack values get moved
down one word. In step (3), the optimized frame has been built, and the
alignment word contains the value 2 as a hint that the zap value also needs to
be popped from the stack on return.&lt;/p&gt;
&lt;h2 id=&quot;deoptimization&quot;&gt;Deoptimization&lt;/h2&gt;
&lt;p&gt;If an optimized function needs to deoptimize, then it’s frame needs to be
translated into several output frames, since a single optimized function may
contain many inlined functions as well. We end up with one &lt;code&gt;InputFrame&lt;/code&gt; and
several &lt;code&gt;OutputFrames&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let’s take an optimized function with no arguments that deopts on entry. The
function has two spill slots, one for the alignment word. The deopt process is
begun with a call to a function that pushes a Bailout ID. The
deoptimization function then pushes registers to the stack and prepares to
create a &lt;code&gt;Deoptimizer&lt;/code&gt; object.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;drawings/frame-deopt.png&quot; width=600&gt;&lt;/center&gt;

&lt;p&gt;The function has deopted, and is preparing to create the Deoptimizer object. All
of the necessary information is on the stack. This information is used to build the Deoptimizer. We then unwind the whole
stack, copying all the registers and then the frame to the input
&lt;code&gt;FrameDescription&lt;/code&gt; object allocated when the Deoptimizer was created. At this
point we go to C++ and compute all of the output frames. After this, 
we check the alignment word, and pop off the alignment “zap
value” if it’s present (not in the example above). We end up at a completely
empty stack, with no way to do anything or go anywhere, because we’ve even
popped off the return address.&lt;/p&gt;
&lt;p&gt;We loop over all the output frames, pushing their contents to the stack from the
higher (deepest) addresses to the lower (most shallow) addresses:&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;drawings/frame-deopt2.png&quot; width=600&gt;&lt;/center&gt;

&lt;p&gt;The OutputFrames have been computed, and are being copied to the stack in the
appropriate place. Finally, continuation data and register state are propped to
the stack. We’ll pop the registers into place, return to the continuation
address, and finally state and pc are consumed to deposit us rather prettily
into frame N-1.&lt;/p&gt;
&lt;p&gt;With a &lt;code&gt;popad&lt;/code&gt; instruction, we
restore the saved registers to the CPU, then
execute a &lt;code&gt;ret&lt;/code&gt; instruction to pop the continuation address from the stack
and jump to it’s code. The state and pc addresses will be consumed to
appropriately enter unoptimized code at the right point with the right
registers. The stack will gradually unwind correctly.&lt;/p&gt;
&lt;p&gt;The output frames have a different fixed size thanks to the addition of the type
feedback vector in full-code JavaScript frames. Here is a side-by-side
translation of the bottom-most frame in a one argument, non-aligned example
where the output frame has no locals:&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;drawings/frame-deopt3.png&quot; width=600&gt;&lt;/center&gt;

&lt;p&gt;Alternatively, if the bottom-most optimized frame was aligned, we’d have to
remove the alignment zap value and shift values to higher stack addresses
(forgive me for focusing so much on alignment…it was rather a bear):&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;drawings/frame-deopt4.png&quot; width=600&gt;&lt;/center&gt;

&lt;p&gt;An aligned, optimized InputFrame gets replaced on the stack like so. Note that
the output frame is the same as that in the previous unaligned case.&lt;/p&gt;
&lt;h2 id=&quot;on-stack-replacement-osr-&quot;&gt;On Stack Replacement (OSR)&lt;/h2&gt;
&lt;p&gt;If we run a tight loop, we may want to optimize and replace code before we
finish. This means optimizing and installing our optimized frame over the
current frame. In fact, we think of simply appending the new parts of our new
frame to the end of the existing JavaScriptFrame. Optimized frames have spill
slots. These will go right after the locals of the frame already there. The
first job on entry to the optimized code (mid-loop, how exciting!) is to copy
those locals into spill slots where the register allocator can track them.&lt;/p&gt;
&lt;p&gt;I altered the OSR entry point to shift those locals up one word on the stack,
overwriting the vector slot from the unoptimized frame. My first approach, which
ended in a hail of mysterious test failures was to leave the vector in place,
and try to get the optimizing compiler to treat it as an “extra” spill
slot. This became very complicated. For one thing, the deoptimizer had to figure
out if it was deoptimizing a function with OSR entries or not, and do the right
thing with the “extra” word in the former case. Also, Crankshaft optimized functions with an OSR
entry can later be entered from the start, and this starting prologue would have
to push an extra dummy value in order to remain in sync with the offsets to
locals and spill slots
established at the OSR entry point. Life was way better when I abandoned this approach!&lt;/p&gt;
&lt;p&gt;Consider also that optimized frames want to be aligned, so the replacement of
code to use OSR also means moving the existing parts of the frame. Here is an
example, showing the unoptimized stack on the left, and the optimized one on the
right. In the before/after diagram, note that the fixed
part gets smaller with the removal of the vector in the optimized frame:&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;drawings/frame-osr.png&quot; width=600&gt;&lt;/center&gt;

&lt;h2 id=&quot;virtual-deoptimization-for-the-debugger&quot;&gt;Virtual deoptimization for the debugger&lt;/h2&gt;
&lt;p&gt;We have a test &lt;a href=&quot;https://chromium.googlesource.com/v8/v8.git/+/master/test/mjsunit/debug-evaluate-locals-optimized.js&quot;&gt;&lt;strong&gt;debug-evaluate-locals-optimized.js&lt;/strong&gt;&lt;/a&gt; which verifies that the
debugger can interpret locals and arguments of all functions on the stack, even
if some of the functions are optimized. The example sets up a series of calls
from function &lt;code&gt;f&lt;/code&gt; down to function &lt;code&gt;h&lt;/code&gt; and invokes the debugger in
function &lt;code&gt;h&lt;/code&gt; to verify expected values.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Function  Locals           Notes
f         a4 = 9, b4 = 10  call g1 (inlined in f, argument adapted)
g1        a3 = 7, b3 = 8   call g2 (inlined in f, constructor frame and
                                    argument adapted)
g2        a2 = 5, b2 = 6   call g3 (inlined in f)
g3        a1 = 3, b1 = 4   call h (not inlined)
h         a0 = 1, b0 = 2   breakpoint&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The deoptimization infrastructure is used by the debugger to compute and store
these local values in a data structure for later perusal. We “deoptimize”
function &lt;code&gt;f&lt;/code&gt; without actually doing so, but only to harvest the output
frames created in a buffer from that process. &lt;code&gt;f&lt;/code&gt; decomposes into 7 output
frames. Here is the input frame layed out on the stack from the call 
&lt;code&gt;f(4, 11, 12)&lt;/code&gt;, and on the right is the bottommost output frame representing
the unoptimized function &lt;code&gt;f&lt;/code&gt;:&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;drawings/debug-example-f.png&quot; width=600&gt;&lt;/center&gt;

&lt;p&gt;The locals and parameters for the full code frame of &lt;code&gt;f&lt;/code&gt; can be queried
according to known frame layouts.&lt;/p&gt;
&lt;p&gt;Note the literal &lt;code&gt;g1&lt;/code&gt;, which is on the stack, not part of the locals, but simply
an expression saved before the call out to g1. Below are the remaining interesting
OutputFrame data structures, one each for g1, g2 and g3. In the g1 frame, I expected to
see a literal expression for the call to g2 on the stack and was initially
worried about a bug. However g2 is called with &lt;code&gt;new g2(...)&lt;/code&gt;, and constructor calls
don’t push an expression onto the stack before the call.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;drawings/debug-example-rest.png&quot; width=700&gt;&lt;/center&gt;

&lt;p&gt;&lt;code&gt;g2&lt;/code&gt; is called with 3 arguments, but it only accepts one so an arguments adaptor
frame is inserted (not displayed here). &lt;code&gt;g3&lt;/code&gt; is called with three arguments as
expected, so no adaptor frame is inserted. In total, 7 OutputFrames are computed:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;f&lt;/li&gt;
&lt;li&gt;arguments adaptor&lt;/li&gt;
&lt;li&gt;g1&lt;/li&gt;
&lt;li&gt;constructor frame&lt;/li&gt;
&lt;li&gt;arguments adaptor&lt;/li&gt;
&lt;li&gt;g2&lt;/li&gt;
&lt;li&gt;g3&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now we start copying this information into a data structure for debugging. First
we examine the frame for &lt;code&gt;g3&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Although my changes in the deoptimizer resulted in correct OutputFrames, the
interpretation was broken. I had to change the &lt;code&gt;FrameDescription&lt;/code&gt; class to
return local offsets correctly according to whether it was describing an
&lt;code&gt;OPTIMIZED&lt;/code&gt; frame or a &lt;code&gt;JAVA_SCRIPT&lt;/code&gt; frame. This would correctly reflect the
variation I’ve introduced with the type feedback vector. With that change made,
the test passes, finding all of the local variables with their correct values.&lt;/p&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Well, this picture shows I get what I want if the system in is place:&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;drawings/frame-vector.png&quot; width=600&gt;&lt;/center&gt;

&lt;p&gt;But how about performance? It looks good for most benchmarks, but there are a
few SunSpider tests that are short enough that we don’t manage to run optimized
code, and there is a net loss because our unmanaged frames are 1 word bigger. I
do have to pay for that. Before making this part of the tree, I’ll need to
validate that the cost is worthwhile when considering the type vector passage as
a whole. On the whole, I’m optimistic.&lt;/p&gt;
&lt;p&gt;My changelist for the work on all
platforms is &lt;a href=&quot;https://codereview.chromium.org/942513002/&quot;&gt;here&lt;/a&gt;. Thank you for
following this meandering course through some V8 internals :).&lt;/p&gt;
</description>
    </item>
  </channel>
</rss>